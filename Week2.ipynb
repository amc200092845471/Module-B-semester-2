{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5190b54c-d49c-4d6f-80cc-22555336a9cd",
   "metadata": {},
   "source": [
    "# Week 2 - Preprocessing, part 2\n",
    "\n",
    "# 1. Lesson: None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4e5ff-b05f-4ef2-96f1-49dcb5beb158",
   "metadata": {},
   "source": [
    "# 2. Weekly graph question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad37e29-6e84-41fa-886d-abc1312213ab",
   "metadata": {},
   "source": [
    "The Storytelling With Data book mentions planning on a \"Who, What, and How\" for your data story.  Write down a possible Who, What, and How for your data, using the ideas in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c862fc32-1297-4587-b9da-4b0642ab3e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "898eb327-aefd-4ac0-b95a-92b616a2181b",
   "metadata": {},
   "source": [
    "# 3. Homework - work with your own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe925521-979f-4983-8d85-8db8d1316e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14836788-b235-4cd4-b94d-5f749c6141a8",
   "metadata": {},
   "source": [
    "This week, you will do the same types of exercises as last week, but you should use your chosen datasets that someone in your class found last semester. (They likely will not be the particular datasets that you found yourself.)\n",
    "\n",
    "### Here are some types of analysis you can do  Use Google, documentation, and ChatGPT to help you:\n",
    "\n",
    "- Summarize the datasets using info() and describe()\n",
    "\n",
    "- Are there any duplicate rows?\n",
    "\n",
    "- Are there any duplicate values in a given column (when this would be inappropriate?)\n",
    "\n",
    "- What are the mean, median, and mode of each column?\n",
    "\n",
    "- Are there any missing or null values?\n",
    "\n",
    "    - Do you want to fill in the missing value with a mean value?  A value of your choice?  Remove that row?\n",
    "\n",
    "- Identify any other inconsistent data (e.g. someone seems to be taking an action before they are born.)\n",
    "\n",
    "- Encode any categorical variables (e.g. with one-hot encoding.)\n",
    "\n",
    "### Conclusions:\n",
    "\n",
    "- Are the data usable?  If not, find some new data!\n",
    "\n",
    "- Do you need to modify or correct the data in some way?\n",
    "\n",
    "- Is there any class imbalance?  (Categories that have many more items than other categories)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab9e6d-18cc-4863-b980-3e52f581763a",
   "metadata": {},
   "source": [
    "# 4. Storytelling With Data graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1911148d-9df6-4b33-a875-8c96408ec834",
   "metadata": {},
   "source": [
    "Just like last week: choose any graph in the Introduction of Storytelling With Data. Use matplotlib to reproduce it in a rough way. I don't expect you to spend an enormous amount of time on this; I understand that you likely will not have time to re-create every feature of the graph. However, if you're excited about learning to use matplotlib, this is a good way to do that. You don't have to duplicate the exact values on the graph; just the same rough shape will be enough.  If you don't feel comfortable using matplotlib yet, do the best you can and write down what you tried or what Google searches you did to find the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b2888f9-3700-45ab-9829-6a5372106f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/teamincribo/cyber-security-attacks?dataset_version_number=20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.03M/5.03M [00:00<00:00, 10.0MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from Beth: C:\\Users\\alexa\\.cache\\kagglehub\\datasets\\katehighnam\\beth-dataset\\versions\\3\\labelled_2021may-ip-10-100-1-105-dns.csv\n",
      "Loading from Beth: C:\\Users\\alexa\\.cache\\kagglehub\\datasets\\katehighnam\\beth-dataset\\versions\\3\\labelled_2021may-ip-10-100-1-186-dns.csv\n",
      "Loading from Beth: C:\\Users\\alexa\\.cache\\kagglehub\\datasets\\katehighnam\\beth-dataset\\versions\\3\\labelled_2021may-ip-10-100-1-26-dns.csv\n",
      "Loading from Beth: C:\\Users\\alexa\\.cache\\kagglehub\\datasets\\katehighnam\\beth-dataset\\versions\\3\\labelled_2021may-ip-10-100-1-4-dns.csv\n",
      "Loading from Beth: C:\\Users\\alexa\\.cache\\kagglehub\\datasets\\katehighnam\\beth-dataset\\versions\\3\\labelled_2021may-ip-10-100-1-95-dns.csv\n",
      "Loading from Beth: C:\\Users\\alexa\\.cache\\kagglehub\\datasets\\katehighnam\\beth-dataset\\versions\\3\\labelled_2021may-ubuntu-dns.csv\n",
      "Loading from Cribo: C:\\Users\\alexa\\.cache\\kagglehub\\datasets\\teamincribo\\cyber-security-attacks\\versions\\20\\cybersecurity_attacks.csv\n",
      "Null values:\n",
      " Action Taken                259\n",
      "Alerts/Warnings           20326\n",
      "Anomaly Scores              259\n",
      "Attack Signature            259\n",
      "Attack Type                 259\n",
      "Destination IP Address      259\n",
      "Destination Port            259\n",
      "DestinationIP             40000\n",
      "Device Information          259\n",
      "DnsAnswer                 40194\n",
      "DnsAnswerTTL              40194\n",
      "DnsOpCode                 40000\n",
      "DnsQuery                  40000\n",
      "DnsQueryClass             40000\n",
      "DnsQueryNames             40000\n",
      "DnsQueryType              40000\n",
      "DnsResponseCode           40000\n",
      "Firewall Logs             20220\n",
      "Geo-location Data           259\n",
      "IDS/IPS Alerts            20309\n",
      "Log Source                  259\n",
      "Malware Indicators        20259\n",
      "Network Segment             259\n",
      "NumberOfAnswers           40000\n",
      "Packet Length               259\n",
      "Packet Type                 259\n",
      "Payload Data                259\n",
      "Protocol                    259\n",
      "Proxy Information         20110\n",
      "SensorId                  40000\n",
      "Severity Level              259\n",
      "Source IP Address           259\n",
      "Source Port                 259\n",
      "SourceIP                  40000\n",
      "Timestamp                     0\n",
      "Traffic Type                259\n",
      "User Information            259\n",
      "evil                      40000\n",
      "sus                       40000\n",
      "dtype: int64\n",
      "Combined Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 40259 entries, 0 to 41613\n",
      "Data columns (total 39 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Action Taken            40000 non-null  float64\n",
      " 1   Alerts/Warnings         19933 non-null  float64\n",
      " 2   Anomaly Scores          40000 non-null  float64\n",
      " 3   Attack Signature        40000 non-null  float64\n",
      " 4   Attack Type             40000 non-null  float64\n",
      " 5   Destination IP Address  40000 non-null  float64\n",
      " 6   Destination Port        40000 non-null  float64\n",
      " 7   DestinationIP           259 non-null    float64\n",
      " 8   Device Information      40000 non-null  float64\n",
      " 9   DnsAnswer               65 non-null     float64\n",
      " 10  DnsAnswerTTL            65 non-null     float64\n",
      " 11  DnsOpCode               259 non-null    float64\n",
      " 12  DnsQuery                259 non-null    float64\n",
      " 13  DnsQueryClass           259 non-null    float64\n",
      " 14  DnsQueryNames           259 non-null    float64\n",
      " 15  DnsQueryType            259 non-null    float64\n",
      " 16  DnsResponseCode         259 non-null    float64\n",
      " 17  Firewall Logs           20039 non-null  float64\n",
      " 18  Geo-location Data       40000 non-null  float64\n",
      " 19  IDS/IPS Alerts          19950 non-null  float64\n",
      " 20  Log Source              40000 non-null  float64\n",
      " 21  Malware Indicators      20000 non-null  float64\n",
      " 22  Network Segment         40000 non-null  float64\n",
      " 23  NumberOfAnswers         259 non-null    float64\n",
      " 24  Packet Length           40000 non-null  float64\n",
      " 25  Packet Type             40000 non-null  float64\n",
      " 26  Payload Data            40000 non-null  float64\n",
      " 27  Protocol                40000 non-null  float64\n",
      " 28  Proxy Information       20149 non-null  float64\n",
      " 29  SensorId                259 non-null    float64\n",
      " 30  Severity Level          40000 non-null  float64\n",
      " 31  Source IP Address       40000 non-null  float64\n",
      " 32  Source Port             40000 non-null  float64\n",
      " 33  SourceIP                259 non-null    float64\n",
      " 34  Timestamp               40259 non-null  float64\n",
      " 35  Traffic Type            40000 non-null  float64\n",
      " 36  User Information        40000 non-null  float64\n",
      " 37  evil                    259 non-null    float64\n",
      " 38  sus                     259 non-null    float64\n",
      "dtypes: float64(39)\n",
      "memory usage: 12.3 MB\n"
     ]
    }
   ],
   "source": [
    "# import kagglehub \n",
    "# import os\n",
    "# import numpy as np \n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "# #downloads the dataset from kagglehub\n",
    "# beth_dataset = kagglehub.dataset_download(\"katehighnam/beth-dataset\")\n",
    "# print(\"Path to dataset files:\", beth_dataset)\n",
    "\n",
    "# #dns dataset\n",
    "# dns_dataset =[]\n",
    "# #I want to test out if I can join all the dataset that contains dns in it\n",
    "# for root, dirs, files in os.walk(beth_dataset):\n",
    "#     for file in files:\n",
    "#             if file.endswith(\".csv\") and \"dns\" in file.lower():\n",
    "#                   beth_dataset_path = os.path.join(root,file)\n",
    "#                   print(f\"Loading: {beth_dataset_path}\")\n",
    "#                   df = pd.read_csv(beth_dataset_path)\n",
    "#                   dns_dataset.append(df)\n",
    "# combined_dns = pd.concat(dns_dataset, ignore_index=True)\n",
    "\n",
    "# #assuming the dataset is named \"labelled_2021may-ip-10-100-1-4-dns.csv\" \n",
    "# # beth_dataset_path_to_file = os.path.join(beth_dataset,\"labelled_2021may-ip-10-100-1-4-dns.csv\")\n",
    "# # print(beth_dataset_path_to_file) \n",
    "# # beth_dataset_raw = pd.read_csv(beth_dataset_path_to_file)\n",
    "\n",
    "# #print out the head() and info()\n",
    "# combined_dns.info()\n",
    "# combined_dns.head()\n",
    "# combined_dns.columns.tolist()\n",
    "\n",
    "# #Now checking histograms\n",
    "# combined_dns.hist(figsize=(15,13), bins=30)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# #checking if there is any duplicate rows\n",
    "# combined_dns.duplicated()\n",
    "# combined_dns.drop_duplicates()\n",
    "# #check if there are null values in any of the dataset\n",
    "# count_null = combined_dns.isnull().sum()\n",
    "# print(count_null)\n",
    "# combined_dns.isnull().sum()[combined_dns.isnull().sum() > 0]\n",
    "# #Since there are so many categorical features I want to encode them\n",
    "# cat_cols = combined_dns.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "# encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "# encoded_array = encoder.fit_transform(combined_dns[cat_cols])\n",
    "# combined_dns[cat_cols] = encoded_array\n",
    "# combined_dns.info()\n",
    "# #then we will move on to remove features that are clearly not useful\n",
    "# #beth_dataset_clean = \n",
    "# #getting the mean,meadian,and mode of each column\n",
    "# combined_dns.head()\n",
    "# print(combined_dns.head())\n",
    "# combined_dns.mean()\n",
    "# combined_dns.median()\n",
    "# combined_dns.mode()\n",
    "# # beth_dataset_raw.mean()\n",
    "# # beth_dataset_raw.median()\n",
    "# # beth_dataset_raw.mode()\n",
    "\n",
    "# #checking for null values\n",
    "# # beth_dataset_raw.isnull()\n",
    "\n",
    "import kagglehub \n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# --- Download both datasets ---\n",
    "beth_dataset = kagglehub.dataset_download(\"katehighnam/beth-dataset\")\n",
    "cribo_dataset = kagglehub.dataset_download(\"teamincribo/cyber-security-attacks\")\n",
    "\n",
    "# --- Load DNS-related CSVs from Beth dataset ---\n",
    "dns_dataset = []\n",
    "for root, dirs, files in os.walk(beth_dataset):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\") and \"dns\" in file.lower():\n",
    "            file_path = os.path.join(root, file)\n",
    "            print(f\"Loading from Beth: {file_path}\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            dns_dataset.append(df)\n",
    "combined_dns = pd.concat(dns_dataset, ignore_index=True)\n",
    "\n",
    "# --- Load all CSVs from Cribo dataset ---\n",
    "cribo_dataframes = []\n",
    "for root, dirs, files in os.walk(cribo_dataset):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            print(f\"Loading from Cribo: {file_path}\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            cribo_dataframes.append(df)\n",
    "combined_cribo = pd.concat(cribo_dataframes, ignore_index=True)\n",
    "\n",
    "# --- Align columns across both datasets ---\n",
    "dns_cols = set(combined_dns.columns)\n",
    "cribo_cols = set(combined_cribo.columns)\n",
    "all_cols = sorted(list(dns_cols.union(cribo_cols)))\n",
    "\n",
    "# Add missing columns to each DataFrame\n",
    "for col in all_cols:\n",
    "    if col not in combined_dns.columns:\n",
    "        combined_dns[col] = np.nan\n",
    "    if col not in combined_cribo.columns:\n",
    "        combined_cribo[col] = np.nan\n",
    "\n",
    "# Reorder columns to match\n",
    "combined_dns = combined_dns[all_cols]\n",
    "combined_cribo = combined_cribo[all_cols]\n",
    "\n",
    "# --- Combine both datasets ---\n",
    "combined_all = pd.concat([combined_dns, combined_cribo], ignore_index=True)\n",
    "\n",
    "# --- Encode categorical columns ---\n",
    "cat_cols = combined_all.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "combined_all[cat_cols] = encoder.fit_transform(combined_all[cat_cols])\n",
    "\n",
    "# --- Drop duplicates, check nulls ---\n",
    "combined_all.drop_duplicates(inplace=True)\n",
    "print(\"Null values:\\n\", combined_all.isnull().sum())\n",
    "print(\"Combined Dataset Info:\")\n",
    "combined_all.info()\n",
    "\n",
    "# --- Save the combined dataset ---\n",
    "combined_all.to_csv(\"combined_beth_cribo_dataset.csv\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
